{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerías Necesarias\n",
    "Importar las librerías necesarias como pandas, numpy, matplotlib, seaborn, sklearn, y nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/izluis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/izluis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  # Librería para manipulación de datos\n",
    "import numpy as np  # Librería para operaciones numéricas\n",
    "import matplotlib.pyplot as plt  # Librería para visualización de datos\n",
    "import seaborn as sns  # Librería para visualización de datos\n",
    "from sklearn.model_selection import train_test_split  # Función para dividir los datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convertir una colección de documentos de texto en una matriz de tokens\n",
    "from sklearn.naive_bayes import MultinomialNB  # Clasificador Naive Bayes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Métricas para evaluar el rendimiento del modelo\n",
    "import nltk  # Librería para procesamiento de lenguaje natural\n",
    "from nltk.corpus import stopwords  # Lista de palabras vacías en varios idiomas\n",
    "from nltk.tokenize import word_tokenize  # Tokenización de palabras\n",
    "\n",
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer Archivos de Datos\n",
    "Leer los archivos cellphones_train.json y cellphones_test.json y crear los conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           categories                                id        klass  \\\n",
       " 0  [ \"DEVICE\", \"OS\" ]  d0fb202ea47c3cad448ea4e5e31bd404  information   \n",
       " 1        [ \"MOBILE\" ]  d76f590d27f8eb415696e945f16b7e1c      neutral   \n",
       " 2            [ \"OS\" ]  e345247011f2e69ca73ee5493c35e599     positive   \n",
       " 3            [ \"OS\" ]  550f607acddf50d8dd515a750b994820  information   \n",
       " 4        [ \"MOBILE\" ]  29e388d45111200b6535f1367c501461      neutral   \n",
       " \n",
       "                                                 text     type  \n",
       " 0  RT @matukpuntocom: Xperia Play se quedará sin ...   Xperia  \n",
       " 1  RT @AlbertoCiurana: Hay gente que se desconect...  Celular  \n",
       " 2  @loneliest_star siiiiiiiiiiiiiiii!!! y ya podr...  Android  \n",
       " 3  El otro día soné que desarrollaba Apps para An...  Android  \n",
       " 4  Escuchandola con Aristegui, lleva amistad BUEN...  Celular  ,\n",
       "      categories                                id        klass  \\\n",
       " 0  [ \"MOBILE\" ]  0b4783023036948eed8f9f1151b708d8  information   \n",
       " 1  [ \"MOBILE\" ]  fee4741dac44bf014b29c51b8d624d82  information   \n",
       " 2  [ \"MOBILE\" ]  8b2e285af81524ce44647a9286defe9d  information   \n",
       " 3  [ \"MOBILE\" ]  26e0812bea0f10f5353f06ff94f9d270  information   \n",
       " 4  [ \"MOBILE\" ]  80869ae712f24a387a212e315ebe97c2  information   \n",
       " \n",
       "                                                 text      type  \n",
       " 0  Ya se volvió a ir la luz, quinto o sexto apagó...   Celular  \n",
       " 1  @clauditafigue todo bien hasta ahora! :) el nú...   Celular  \n",
       " 2  Felicidades a los policías,y gracias por acept...  Teléfono  \n",
       " 3  telcel murió o algo así o porque mi teléfono e...  Teléfono  \n",
       " 4  @diego_alva8 ya no tengo celular hablamos por ...   Celular  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json  # Librería para trabajar con archivos JSON\n",
    "\n",
    "# Función para leer múltiples objetos JSON de un archivo\n",
    "def read_json_objects(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Leer archivos JSON\n",
    "train_data = read_json_objects('cellphones_train.json')\n",
    "test_data = read_json_objects('cellphones_test.json')\n",
    "\n",
    "# Crear DataFrames de pandas\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# Mostrar las primeras filas de los DataFrames para verificar la carga correcta\n",
    "df_train.head(), df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesar Texto\n",
    "Realizar preprocesamiento del texto, incluyendo la eliminación de stopwords, tokenización, y lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/izluis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Aplicar preprocesamiento al conjunto de entrenamiento y prueba\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Mostrar las primeras filas de los DataFrames para verificar el preprocesamiento\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Lematizar tokens\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/stem/wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/stem/wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(form, pos, check_exceptions)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/wordnet.py:1214\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# map from WordNet 3.0 for OMW data\u001b[39;00m\n\u001b[0;32m-> 1214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap30 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_wn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# Language data attributes\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg_attrs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexe\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/wordnet.py:1282\u001b[0m, in \u001b[0;36mWordNetCorpusReader.map_wn\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_to_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/wordnet.py:1254\u001b[0m, in \u001b[0;36mWordNetCorpusReader.map_to_one\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnomap[version] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits[version] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1254\u001b[0m synset_to_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_to_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m synset_to_one \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m synset_to_many:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/wordnet.py:1238\u001b[0m, in \u001b[0;36mWordNetCorpusReader.map_to_many\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_to_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1238\u001b[0m     sensekey_map1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_sense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m     sensekey_map2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_sense()\n\u001b[1;32m   1240\u001b[0m     synset_to_many \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/wordnet.py:1230\u001b[0m, in \u001b[0;36mWordNetCorpusReader.index_sense\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ixreader\u001b[38;5;241m.\u001b[39mopen(fn) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m   1229\u001b[0m     sensekey_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msensekey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:1236\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:1229\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1229\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[1;32m   1231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m line\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:1188\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.readline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1184\u001b[0m new_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read(readsize)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we're at a '\\r', then read one extra character, since\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# it might be a '\\n', to get the proper line ending.\u001b[39;00m\n\u001b[0;32m-> 1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_chars \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mnew_chars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1189\u001b[0m     new_chars \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1191\u001b[0m chars \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chars\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk  # Librería para procesamiento de lenguaje natural\n",
    "from nltk.stem import WordNetLemmatizer  # Lematización de palabras\n",
    "\n",
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función para preprocesar texto\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text)\n",
    "    # Eliminar stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('spanish')]\n",
    "    # Lematizar tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar preprocesamiento al conjunto de entrenamiento y prueba\n",
    "df_train['processed_text'] = df_train['text'].apply(preprocess_text)\n",
    "df_test['processed_text'] = df_test['text'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar las primeras filas de los DataFrames para verificar el preprocesamiento\n",
    "df_train.head(), df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear Gráficas de Palabras Comunes\n",
    "Crear gráficas que muestren las palabras más comunes para cada una de las etiquetas usando matplotlib y seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Función para obtener las palabras más comunes\n",
    "def obtener_palabras_comunes(textos, n=10):\n",
    "    todas_las_palabras = ' '.join(textos).split()\n",
    "    contador_palabras = Counter(todas_las_palabras)\n",
    "    return contador_palabras.most_common(n)\n",
    "\n",
    "# Obtener palabras más comunes para cada etiqueta\n",
    "etiquetas = df_train['label'].unique()\n",
    "palabras_comunes_por_etiqueta = {}\n",
    "\n",
    "for etiqueta in etiquetas:\n",
    "    textos_etiqueta = df_train[df_train['label'] == etiqueta]['processed_text']\n",
    "    palabras_comunes_por_etiqueta[etiqueta] = obtener_palabras_comunes(textos_etiqueta)\n",
    "\n",
    "# Crear gráficas de palabras comunes\n",
    "fig, axes = plt.subplots(len(etiquetas), 1, figsize=(10, 5 * len(etiquetas)))\n",
    "\n",
    "for i, etiqueta in enumerate(etiquetas):\n",
    "    palabras, frecuencias = zip(*palabras_comunes_por_etiqueta[etiqueta])\n",
    "    sns.barplot(x=frecuencias, y=palabras, ax=axes[i])\n",
    "    axes[i].set_title(f'Palabras más comunes para la etiqueta: {etiqueta}')\n",
    "    axes[i].set_xlabel('Frecuencia')\n",
    "    axes[i].set_ylabel('Palabras')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizar Texto\n",
    "Vectorizar el texto usando CountVectorizer y TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Importar TF-IDF Vectorizer\n",
    "\n",
    "# Inicializar CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(df_train['processed_text'])\n",
    "X_test_counts = count_vectorizer.transform(df_test['processed_text'])\n",
    "\n",
    "# Inicializar TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['processed_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['processed_text'])\n",
    "\n",
    "# Mostrar las dimensiones de las matrices resultantes\n",
    "X_train_counts.shape, X_test_counts.shape, X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar Clasificador\n",
    "Entrenar varios clasificadores como Regresión Logística, SVM, Naive Bayes, y Random Forest para predecir la etiqueta del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  # Importar Regresión Logística\n",
    "from sklearn.svm import SVC  # Importar SVM\n",
    "from sklearn.ensemble import RandomForestClassifier  # Importar Random Forest\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, df_train['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar clasificadores\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC()\n",
    "naive_bayes = MultinomialNB()\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Entrenar clasificadores\n",
    "log_reg.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predecir etiquetas en el conjunto de validación\n",
    "y_pred_log_reg = log_reg.predict(X_val)\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "y_pred_naive_bayes = naive_bayes.predict(X_val)\n",
    "y_pred_random_forest = random_forest.predict(X_val)\n",
    "\n",
    "# Evaluar clasificadores\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_log_reg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_log_reg))\n",
    "\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_svm))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_svm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_svm))\n",
    "\n",
    "print(\"\\nNaive Bayes\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_naive_bayes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_naive_bayes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_naive_bayes))\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_random_forest))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_random_forest))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular Accuracy\n",
    "Calcular el accuracy de cada modelo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predecir etiquetas en el conjunto de prueba\n",
    "y_test_log_reg = log_reg.predict(X_test_tfidf)\n",
    "y_test_svm = svm.predict(X_test_tfidf)\n",
    "y_test_naive_bayes = naive_bayes.predict(X_test_tfidf)\n",
    "y_test_random_forest = random_forest.predict(X_test_tfidf)\n",
    "\n",
    "# Calcular accuracy para cada modelo\n",
    "accuracy_log_reg = accuracy_score(df_test['label'], y_test_log_reg)\n",
    "accuracy_svm = accuracy_score(df_test['label'], y_test_svm)\n",
    "accuracy_naive_bayes = accuracy_score(df_test['label'], y_test_naive_bayes)\n",
    "accuracy_random_forest = accuracy_score(df_test['label'], y_test_random_forest)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Accuracy Logistic Regression:\", accuracy_log_reg)\n",
    "print(\"Accuracy Support Vector Machine:\", accuracy_svm)\n",
    "print(\"Accuracy Naive Bayes:\", accuracy_naive_bayes)\n",
    "print(\"Accuracy Random Forest:\", accuracy_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentación de Preprocesamiento y Modelos\n",
    "Incluir un texto que documente los diferentes tipos de preprocesamiento usados, el tipo de vectorizador, el modelo para clasificar y su accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentación de Preprocesamiento y Modelos\n",
    "\n",
    "\"\"\"\n",
    "En esta sección, se documentan los diferentes tipos de preprocesamiento utilizados, el tipo de vectorizador, el modelo para clasificar y su accuracy.\n",
    "\n",
    "## Preprocesamiento\n",
    "\n",
    "1. **Conversión a minúsculas**: Todos los textos se convierten a minúsculas para asegurar la uniformidad.\n",
    "2. **Tokenización**: Los textos se dividen en palabras individuales (tokens).\n",
    "3. **Eliminación de stopwords**: Se eliminan las palabras vacías (stopwords) en español utilizando la lista de NLTK.\n",
    "4. **Lematización**: Se aplica lematización a los tokens para reducirlos a su forma base.\n",
    "\n",
    "## Vectorización\n",
    "\n",
    "Se utilizaron dos tipos de vectorizadores para convertir los textos preprocesados en matrices de características:\n",
    "\n",
    "1. **CountVectorizer**: Convierte una colección de documentos de texto en una matriz de tokens contados.\n",
    "2. **TfidfVectorizer**: Convierte una colección de documentos de texto en una matriz de TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "## Modelos de Clasificación\n",
    "\n",
    "Se entrenaron y evaluaron cuatro modelos de clasificación diferentes:\n",
    "\n",
    "1. **Regresión Logística (Logistic Regression)**\n",
    "2. **Máquina de Soporte Vectorial (Support Vector Machine - SVM)**\n",
    "3. **Naive Bayes Multinomial (Multinomial Naive Bayes)**\n",
    "4. **Bosque Aleatorio (Random Forest)**\n",
    "\n",
    "## Resultados de Accuracy\n",
    "\n",
    "Los resultados de accuracy para cada modelo en el conjunto de prueba son los siguientes:\n",
    "\n",
    "- **Logistic Regression**: {accuracy_log_reg}\n",
    "- **Support Vector Machine**: {accuracy_svm}\n",
    "- **Naive Bayes**: {accuracy_naive_bayes}\n",
    "- **Random Forest**: {accuracy_random_forest}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
